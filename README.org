#+title: Bianet Neuron: A Neuron Simulation in Common Lisp

Bianet is an experimental neuron simulation in Common Lisp that models neurons as independent objects, enabling flexible network topologies and dynamic behaviors not possible in traditional neural network frameworks.

* Overview
Bianet treats neurons as independent objects, unlike traditional neural networks where neurons are simple data units. This design enables dynamic, cyclic network topologies where a neuron's output can later affect itself, ideal for experimental AI and neuroscience research. Common Lisp was chosen for its concurrency primitives, speed, and, most of all, flexibility.

** Why Independend Neurons?
Each neuron can have its own transfer function and can be rewired or removed during runtime (training or even inference), enabling novel connectivity patterns not supported by frameworks like TensorFlow or PyTorch.

** Trade-Offs
This approach sacrifices some efficiency (e.g., due to thread contention or distributed communication overhead) but unlocks unique capabilities for research and experimentation.
* Neuron Diagram
#+caption: Bianet Neuron Model
[[./neuron.png]]
The diagram shows a neuron with four components: Input Receiver, Output Transmitter, Modulation Receiver, and Modulation Transmitter, connected by processes like Excited and Modulated.
** Input Receiver
The Input Receiver collects signals from other neurons and processes them through the `Excited` function, which checks if enough signals are received to activate the neuron. A background process monitors excitation and pauses when idle to save resources. The aggregated input is transformed by a transfer function into an output value.
** Output Transmitter
The Output Transmitter sends the neuron's output to downstream neurons via weighted connections, allowing fine-grained control over signal propagation.
** Modulation Receiver
During backpropagation, downstream neurons send modulation signals to the Modulation Receiver. The `Modulated` function checks for sufficient signals, then uses the `Transfer Derivative` to compute the neuron's `Error`, which adjusts outgoing weights and is sent upstream.
** Modulation Transmitter
The Modulation Transmitter propagates the neuron's error to upstream neurons, supporting backpropagation across the network.
* Tests
Tests verify neuron functionality, signal propagation, weight adjustment, and so on. Run them with =make=.

